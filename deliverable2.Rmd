---
title: "Model Planning and Building"
author: "Austin Meyer"
date: "3/12/2019"
output: html_document
---

## Start with the past
  
  Before we go forward with gathering more data we must first load what we did from the last part of our journey. You will not see this because I am hidding it from you, but if you test at home you will see it.
  
```{r echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, results='hide'}
include <- function(library_name){
  if( !(library_name %in% installed.packages()) )
    install.packages(library_name) 
  library(library_name, character.only=TRUE)
}
include("tidyverse")
include("knitr")
include("caret")
include("rvest")
include("hexbin")
purl("deliverable1.Rmd", output = "part1.r")
source("part1.r")
```

## Time to add more data

  To start this part of my investigation I will be looking for another data set to supplement our previous data set. The catch to this one is that it will be from another source.Two parts of the data will be from Gallup. These two polls will help us figure out what people are thinking when choosing candidets. The other is from the NYTimes and gives us the results of the election so that we can reference it through out the report.
  
  Before we go into our models we must first make functions to easily grab and sort our data.
  
```{r}
read_election_outcome <- function(url) {
  
  html_full <- read_html(url)
  
  outcome <- html_full %>% html_nodes("tbody")

  percent_won <-  outcome %>%
                  html_nodes("span.eln-percent") %>%
                  html_text()
  candid_name <- outcome %>%
                  html_nodes("span.eln-name") %>%
                  html_text()
  party <- outcome %>%
                  html_nodes("span.eln-party") %>%
                  html_text()
  
  results <- tibble( percent_won=percent_won, candidite_name=candid_name, party=party)
}

read_congress_approval <- function(url, div)
{
  html_full <- read_html(url)
  
  outcome <- html_full %>% html_node(div)
  tbody_data <- outcome %>% html_nodes("tbody")
  tr_data <- tbody_data %>% html_nodes("tr")
  
  poll_date <- tbody_data %>%
                html_nodes("th[scope=row]") %>%
                html_text()
  
  approve <- tbody_data %>%
                html_nodes("td[data-th='Yes, deserves']") %>%
                html_text()
  
  disapprove <- outcome %>%
                html_nodes("td[data-th='No, does not']") %>%
                html_text()
  
  no_opinion <- outcome %>%
                html_nodes("td[data-th='No opinion']") %>%
                html_text()
  
  results <- tibble(poll_date=poll_date, approve=approve, disapprove=disapprove, no_opinion=no_opinion)
}

```

  After initializing our helpers we can now use them to gather our data from the websites as seen bellow.

```{r}
# Then using the funciton we made above, we will gather the data from the website.
election_result <- read_election_outcome(url = "https://www.nytimes.com/interactive/2018/11/06/us/elections/results-house-elections.html")
other_congress_feels <- read_congress_approval(url = "https://news.gallup.com/poll/1600/congress-public.aspx", div = "div#ite-235073")
your_congress_feels <- read_congress_approval(url = "https://news.gallup.com/poll/1600/congress-public.aspx", div = "div#ite-235070")
```

## Clean up

  The only item that looks like it needs a good scrubing is the date. So we are going to turn it into POSIX standard time so that when we work with the data we will not get confused and it is all standardized. To make the dates simpler we are going to only record the date when the poll started polling.
  
```{r}
your_congress_feels$poll_date <- sub("-.*", "", your_congress_feels$poll_date)
other_congress_feels$poll_date <- sub("-.*", "", other_congress_feels$poll_date)

# Row 42 has an issue and doesn't follow the same format as the others.
# We are going to have to change it manually
other_congress_feels$poll_date[other_congress_feels$poll_date == "1992 Jul 31 Aug 2"] <- strtrim("1992 Jul 31 Aug 2", nchar("1992 Jul 31 Aug 2")-6)

your_congress_feels$poll_date <- as.POSIXlt(parse_datetime(your_congress_feels$poll_date, format="%Y %b %d", locale = locale("en")))
other_congress_feels$poll_date <- as.POSIXlt(parse_datetime(other_congress_feels$poll_date, format="%Y %b %d", locale = locale("en")))
```

  Also found some areas were our data could be cleaned better. We should first turn the states and districts into factors.
```{r}
national_data$state <- as.factor(national_data$state)
national_data$district <- as.factor(national_data$district)
```

# Modeling data

  Now that we have acquired our secondary data sources we must now seek out correlations in our data. To do this we will be using the lm function. This makes it easier for us to model data. We will only need the original data set to do this.I want to first model what corelations can be made with the win_probability and the other variables from the national_data set.
  
```{r}
national_model <- lm(national_data, formula=win_probability~state+district+party+incumbent)
summary(national_model)
```
  Now we want to see if the data we chose will be a good model to use in our analysis. To do this we will grab a random amount of data from the set and call it our trained set. This set will be used to see if it can predict the other 30% of the data we did not grab. 

```{r}
set.seed(4567)

index <- createDataPartition(national_data$win_probability, p = 0.70, list=FALSE)

train <- national_data[index, ]
test <- national_data[-index, ]

train_model <- lm(train, formula=win_probability~state+district+party+incumbent)
summary(train_model)
```

  Now that we have our trained set we need to see how it predicts the next set, and graph the results with the actual data. First we must talk about the lost rows. I believe these rows were lost do to certain candiadtes not having a party, or to many dots were placed on the same position. 
  Based on our predictions we can conclud that the columns we chose do help in predicting the winner of an election. When win probability is 1.00 so is national_predictions majority of the time.
  
  Of course predict is not always right. Some predicitons predict a winner, but they actually lose based on our models data. Some predictors such as states and districts probably confuse the prediction.
  
```{r}
national_predictions <- predict(train_model, test)
ggplot(data=test, aes(x=national_predictions, y=win_probability )) + geom_point(alpha = 0.01)
```

## Conclusion

  The model developed above shows that we can predict the outcome of an election with some certainty, but not a whole lot. Even though the p values of our variables are very low, the R-squared value is also low. This draws my conclusion on this model that we could use another set of our data to use as varibles.
  
  On the other hand the varible that I thought would be the most meaning full was if the congress person was an incumbant or not. This was not the case. The incumbancy is still valuable, but not as many of the others, such as certain states and districts.
  
  This may help me discover what actually has us choosing the same congress people over and over again the next time.